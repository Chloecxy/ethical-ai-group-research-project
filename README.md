# **Racism in AI Chatbots â€“ Group Project**

A university group project on researching racism and bias in AI chatbots. The project explores the background of racial bias in conversational AI, formulates the problem, and proposes a structured solution with supporting artifacts, including personas, pseudocode, and a Figma prototype.

## Background

AI chatbots are widely used across industries, but they often reproduce harmful racial biases found in training data or design flaws. This raises serious ethical and social concerns, making it vital to study and address such issues.

## Problem Statement

Current AI chatbots are often programmed without sufficient attention to mitigating racial bias and ensuring ethical values, leading to the risk of spreading discriminatory content.

## Proposed Solution

- Detection of Racist Language
- Rephrasing & Handling Racist Replies
- Reporting System
- Tracking & History

## Pseudocode
```
GET input from user AND pass to chatbot
FOR each word in response
    CHECK word against bias database
    APPLY AI text classification

IF no racist words
    DISPLAY response AND restart
ELSE
    DISPLAY "racist response detected"
    (Virtue Ethics Honesty Principle)
```

## Personas & Scenarios

We design user personas from diverse backgrounds to test chatbot interactions in realistic scenarios, ensuring inclusivity and fairness.

## Prototype (Figma)

A Figma prototype demonstrates the chatbotâ€™s interface, bias-handling features, and reporting system.

### ðŸ”— Link to Figma prototype:
https://www.figma.com/design/ddvKhqUKOA0blT7MjYFZkQ/1055-prototype?node-id=24-1242&t=5Wu0r5I8ygojxkv6-1

## License

This project is licensed under the MIT License â€“ see the LICENSE file for details.